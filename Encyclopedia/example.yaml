Example Subject:
  category: "Example category"
  subcategory: "Example subcategory"
  date: 2002/02/02
  links: "https://example.com"
  contents: |-
    Here is some example content.
  subcontents:
     Example Subcontet: Text data of the example subcontent

Mean:
  category: Concepts
  date: 2021/09/05
  contents: |-
      Mean (defined by mu) is the average of a signal calculated by adding all samples and dividing by the number of samples.
      The mean represents the DC value.

Signal to Noise Ratio (SNR):
  category: Concepts
  date: 2021/09/05
  contents: |-
      SNR is defined as the logarithmic power ratio of the signal and noise.

          SNR = 20*log(s(n)/w(n)) = 10*log(s(n)^2/w(n)^2).

Average deviation:
  category: Concepts
  date: 2021/09/05
  contents: |-
      The average devitation of a signal is found by summing the deviations absolute values of all the individual samples and then dividing by number of samples N.
      It describes how far the n samples diviates from the mean.

Standard deviation:
  category: Concepts
  date: 2021/09/05
  contents: |-
      Calculated by squaring each deviation value before dividing by N-1.
      
            SD = sqrt((x[0]-Mean)^2 + (x[1]-Mean)^2 + ... +(x[n]-Mean)^2)

      The standard deviation (defined by sigma) is a measure of how far the signal fluctuates from the mean. It only measures the AC portion of the signal.
      Since a lot of the squares can become zero due to the proximity of the mean, another way to calculate the SD is:

            SD = sqrt((1/N-1)*(sum_of_squares-(sum^2/N)))

Variance:
  category: Concepts
  date: 2021/09/05
  contents: |-
      The variance (defined by sigma squared) is the square of the standard deviation. It represents the power of the fluctiation of the signal from its mean value.
      
Coeficient of variation (CV):
  category: Concepts
  date: 2021/09/05
  contents: |-
      CV is defined as the standard deviation divided by the mean time 100%.
      
Strong law of large numbers:
  category: Concepts
  date: 2021/09/05
  contents: |-
      The large the number of sample N, the smaller the expected error will become when calculating the mean of a signal. The error becomes zero as the N apporaches infinity.
      The typical error calculating the mean of finite set of samples N is:

            Typical error= Standard deviation/(sqrt(N))

      That's why its better when calculating the mean to divide the summation by N-1 instead of N.

Histogram:
  category: Concepts
  date: 2021/09/05
  contents: |-
      The histogram display the number of occurences of each possible value for N ammount of samples.
      Chosing a larger data set results in a smoother histogram. The statistical noise (roughness) of the histogram is inversely proportional to the square root of the number of samples N.
      The histogram can be used to efficiently calculate the mean and standard deviation of very large data sets. They can be calculate by:

            Mean= (1/N)*(i0*H[i0] + i1*H[i1] +...+  in*H[in])

      The "i" is the value of the sample (inside range of possibilities), "H[i]" is the number of occurences for "i" and "n" is the number of samples. 

            Variance= (1/N-1) * (H[i0]*(i0-Mean)^2 + H[i1]*(i1-Mean)^2 +...+ H[in]*(in-Mean)^2)
            Standard deviation= sqrt(Variance)


Probability mass function:
  category: Concepts
  date: 2021/09/05
  contents: |-
      The probability mass function is similar to the histogram, being that the histogram shows the number of occurences for each value and the PMF shows the probability (0 to 1) of that each value to occur.
      To get the PMF just divide the total ammount of samples N used in the histogram by each value of H[i].

Probability density function:
  category: Concepts
  date: 2021/09/05
  contents: |-
      Also called as probability distribution function (PDF) is similar to the probability mass function but instead of describing a set of discrete samples, the PDF is used with continous signals. 
      The area under the PDF curve is equal to one.